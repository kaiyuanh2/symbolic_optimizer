{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61cfa2c6-51d7-4937-a0fd-6c1215b6c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import sympy\n",
    "import symengine as se\n",
    "import functools\n",
    "import itertools\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from error_injection import MissingValueError, SamplingError, Injector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import mutual_info_score, auc, roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize as scipy_min\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize, Bounds, linprog\n",
    "from sympy import Symbol as sb\n",
    "from sympy import lambdify\n",
    "from tqdm.notebook import trange,tqdm\n",
    "from IPython.display import display,clear_output\n",
    "from random import choice\n",
    "\n",
    "import cProfile\n",
    "import time\n",
    "import pstats\n",
    "\n",
    "from dowhy import CausalModel\n",
    "from dowhy import causal_estimators\n",
    "import dowhy.datasets\n",
    "\n",
    "import ginac_module\n",
    "import yep\n",
    "import sys\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "import regex\n",
    "import string\n",
    "from functools import partial\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad50050-5bcb-4dd6-ba95-7785741e8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = cProfile.Profile()\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d93363-2503-40ff-80bb-89db1fe1d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b49ba2d-4a82-4d49-a309-b7ca6aea4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symbol(suffix=''):\n",
    "    global symbol_id\n",
    "    symbol_id += 1\n",
    "    name = f'e{symbol_id}_{suffix}' if suffix else f'e{symbol_id}'\n",
    "    return sympy.Symbol(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511beecc-e868-434f-83f4-bcec9489eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions from causal inference not appeared in zono_reg library\n",
    "\n",
    "symbol_id = -1\n",
    "scaler_symbols = set([sb(f'k{i}') for i in range(100)])\n",
    "linearization_dict = dict()\n",
    "reverse_linearization_dict = dict()\n",
    "\n",
    "\n",
    "def compute_closed_form(X, y):\n",
    "    return np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y))\n",
    "\n",
    "# def sample_data(imputed_datasets, uncert_inds=[], seed=42):\n",
    "#     imp_np = np.array(imputed_datasets)\n",
    "#     if len(uncert_inds) == 0:\n",
    "#         uncert_inds = list(itertools.product(range(imp_np.shape[1]),range(imp_np.shape[2])))\n",
    "#     np.random.seed(seed)\n",
    "#     choices = np.random.choice(np.arange(imp_np.shape[0]), len(uncert_inds), replace=True)\n",
    "#     sample_result = imputed_datasets[0].copy()\n",
    "#     for i, ind in enumerate(uncert_inds):\n",
    "#         sample_result[ind[0]][ind[1]] = imputed_datasets[choices[i]][ind[0]][ind[1]]\n",
    "#     return sample_result\n",
    "\n",
    "def sample_data(uncert_inds, uncertain_attr, X_extended_max, X_extended_min, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    sample_result = []\n",
    "    for u in uncert_inds:\n",
    "        maximum = X_extended_max[u, uncertain_attr]\n",
    "        minimum = X_extended_min[u, uncertain_attr]\n",
    "        sample_result.append(np.random.uniform(minimum, maximum))\n",
    "    return sample_result\n",
    "\n",
    "# X_extended_max[:, uncertain_attr]\n",
    "\n",
    "\n",
    "def linearization(expr_ls):\n",
    "    # processed_expr_ls = [0 for _ in range(len(expr_ls))]\n",
    "    processed_expr_ls = []\n",
    "    print(len(expr_ls))\n",
    "    for expr_id, expr in enumerate(expr_ls):\n",
    "        print(\"Linearization\", expr_id)\n",
    "        # Do not support monomial expr currently, e.g., expr = 1.5*e1. \n",
    "        # At lease two monomials in expr, e.g., expr = 1.5*e1 + 2.\n",
    "        if not(expr.free_symbols):\n",
    "            processed_expr_ls[expr_id] += expr\n",
    "            continue\n",
    "        expr = expr.expand()\n",
    "        processed_expr_ls.append(expr)\n",
    "        # for arg in expr.args:\n",
    "        #     print(arg)\n",
    "        #     if not(arg.free_symbols):\n",
    "        #         processed_expr_ls[expr_id] += arg\n",
    "        #         continue\n",
    "        #     p = arg.as_poly()\n",
    "        #     monomial_exponents = p.monoms()[0]\n",
    "            \n",
    "        #     # only deal with non-linear monomials (order > 2)\n",
    "        #     if sum(monomial_exponents) <= 1:\n",
    "        #         processed_expr_ls[expr_id] += arg\n",
    "        #         continue\n",
    "\n",
    "        #     monomial = sympy.prod(x**k for x, k in zip(p.gens, monomial_exponents) \n",
    "        #                           if not(x in scaler_symbols))\n",
    "        #     # check global substitution dictionary\n",
    "        #     if monomial in linearization_dict:\n",
    "        #         processed_expr_ls[expr_id] += arg.coeff(monomial)*linearization_dict[monomial]\n",
    "        #     else:\n",
    "        #         print(\"not in dict\")\n",
    "        #         found = False\n",
    "        #         subs_monomial = create_symbol()\n",
    "        #         for symb in monomial.free_symbols:\n",
    "        #             if symb in reverse_linearization_dict:\n",
    "        #                 equivalent_monomial = monomial.subs(symb, reverse_linearization_dict[symb])\n",
    "        #                 if equivalent_monomial in linearization_dict:\n",
    "        #                     subs_monomial = linearization_dict[equivalent_monomial]\n",
    "        #                     found = True\n",
    "        #                     break\n",
    "        #         linearization_dict[monomial] = subs_monomial\n",
    "        #         if not(found):\n",
    "        #             reverse_linearization_dict[subs_monomial] = monomial\n",
    "        #         processed_expr_ls[expr_id] += arg.coeff(monomial)*subs_monomial\n",
    "                \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def merge_small_components_pca(expr_ls, budget=10):\n",
    "    if not(isinstance(expr_ls, sympy.Expr)):\n",
    "        expr_ls = sympy.Matrix(expr_ls)\n",
    "    if expr_ls.free_symbols:\n",
    "        center = expr_ls.subs(dict([(symb, 0) for symb in expr_ls.free_symbols]))\n",
    "    else:\n",
    "        return expr_ls\n",
    "    monomials_dict = get_generators(expr_ls)\n",
    "    generators = np.array([monomials_dict[m] for m in monomials_dict])\n",
    "    if len(generators) <= budget:\n",
    "        return expr_ls\n",
    "    monomials = [m for m in monomials_dict]\n",
    "    pca = PCA(n_components=len(generators[0]))\n",
    "    pca.fit(np.concatenate([generators, -generators]))\n",
    "    transformed_generators = pca.transform(generators)\n",
    "    transformed_generator_norms = np.linalg.norm(transformed_generators, axis=1, ord=2)\n",
    "    # from largest to lowest norm\n",
    "    sorted_indices = transformed_generator_norms.argsort()[::-1].astype(int)\n",
    "    sorted_transformed_generators = transformed_generators[sorted_indices]\n",
    "    sorted_monomials = [monomials[idx] for idx in sorted_indices]\n",
    "    new_transformed_generators = np.concatenate([sorted_transformed_generators[:budget], \n",
    "                                                 np.diag(np.sum(np.abs(sorted_transformed_generators[budget:]), \n",
    "                                                                axis=0))])\n",
    "    new_generators = pca.inverse_transform(new_transformed_generators)\n",
    "    new_monomials = sorted_monomials[:budget] + [create_symbol() for _ in range(len(generators[0]))]\n",
    "    \n",
    "    processed_expr_ls = center\n",
    "    for monomial_id in range(len(new_monomials)):\n",
    "        processed_expr_ls += sympy.Matrix(new_generators[monomial_id])*new_monomials[monomial_id]\n",
    "    \n",
    "    return processed_expr_ls\n",
    "\n",
    "\n",
    "def get_vertices(affset):\n",
    "    l = len(affset)\n",
    "    distinct_symbols = set()\n",
    "    for expr in affset:\n",
    "        if not(isinstance(expr, sympy.Expr)):\n",
    "            assert isinstance(expr, int) or isinstance(expr, float)\n",
    "        else:\n",
    "            if distinct_symbols:\n",
    "                distinct_symbols = distinct_symbols.union(expr.free_symbols)\n",
    "            else:\n",
    "                distinct_symbols = expr.free_symbols\n",
    "    distinct_symbols = list(distinct_symbols)\n",
    "    # print(distinct_symbols)\n",
    "    combs = [list(zip(distinct_symbols,list(l))) for l in list(itertools.product([-1, 1], repeat=len(distinct_symbols)))]\n",
    "    res = set()\n",
    "    for assignment in combs:\n",
    "        res.add(tuple([expr.subs(assignment) for expr in affset]))\n",
    "    return(res)\n",
    "\n",
    "def count_matrix_terms(matrix):\n",
    "    total_terms = 0\n",
    "    \n",
    "    # Iterate through each element in the matrix\n",
    "    for i in range(matrix.rows):\n",
    "        for j in range(matrix.cols):\n",
    "            element = matrix[i, j]\n",
    "            \n",
    "            # Count terms in this element\n",
    "            if element.is_Add:\n",
    "                # If the element is an Add expression (has + or - operations)\n",
    "                total_terms += len(element.as_ordered_terms())\n",
    "            elif element != 0:\n",
    "                # If the element is a single term (not zero)\n",
    "                total_terms += 1\n",
    "                \n",
    "    return total_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34dba117-414b-42f5-9090-3e4c7293c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympy_to_ginac_format(expr):\n",
    "    \"\"\"Convert SymPy expression to GiNaC-parseable string format.\"\"\"\n",
    "    # Replace SymPy-specific functions with GiNaC equivalents\n",
    "    s = str(expr)\n",
    "    s = s.replace('**', '^')  # Exponentiation\n",
    "    s = s.replace('sin', 'GiNaC::sin')\n",
    "    s = s.replace('cos', 'GiNaC::cos')\n",
    "    # Add more replacements as needed\n",
    "    return s\n",
    "\n",
    "def sympy_matrix_to_ginac(matrix):\n",
    "    \"\"\"Convert SymPy matrix to GiNaC-parseable list of string format.\"\"\"\n",
    "    flattened_list = list(matrix.flat())\n",
    "    flattened_list = [sympy_to_ginac_format(expr) for expr in flattened_list]\n",
    "    return flattened_list\n",
    "\n",
    "def ginac_matrix_to_sympy(matrix, rows, cols, symbols_dict):\n",
    "    POWER_PATTERN = regex.compile(r'\\^')\n",
    "    matrix_list = []\n",
    "    math_funcs = {\n",
    "        'sin': sympy.sin, \n",
    "        'cos': sympy.cos, \n",
    "        'tan': sympy.tan,\n",
    "        'exp': sympy.exp, \n",
    "        'log': sympy.log, \n",
    "        'sqrt': sympy.sqrt\n",
    "    }\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i * cols + j\n",
    "            print(f\"converting entry {idx} back to SymPy\")\n",
    "            expr_str = POWER_PATTERN.sub('**', matrix[idx])\n",
    "            symengine_expr = se.S(expr_str)\n",
    "            matrix_list.append(symengine_expr)\n",
    "            # row.append(eval(expr_str, {\"__builtins__\": {}}, {**symbols_dict, **math_funcs}))\n",
    "\n",
    "        # matrix_list.append(row)\n",
    "\n",
    "    se_matrix = se.DenseMatrix(rows, cols, matrix_list)\n",
    "\n",
    "    print(\"converting to sympy matrix\")\n",
    "    return sympy.Matrix(rows, cols, [sympy.sympify(se_matrix[i, j]) \n",
    "                                for i in range(rows) \n",
    "                                for j in range(cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad3edc4-8df9-494e-bc34-01aee44057ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ginac_tokens(ginac_expr_str, sympy_symbols):\n",
    "    \"\"\"\n",
    "    A string-based approach to convert GiNaC expressions to SymPy\n",
    "    without any recursion, with support for scientific notation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ginac_expr_str : str\n",
    "        String representation of a GiNaC expression\n",
    "    sympy_symbols : dict\n",
    "        Dictionary mapping symbol names to SymPy symbols\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sympy.Expr\n",
    "        The equivalent SymPy expression\n",
    "    \"\"\"\n",
    "    # Start timing for performance monitoring\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a case-sensitive symbol map to ensure accurate matching\n",
    "    symbol_namespace = {}\n",
    "    for i, (name, symbol) in enumerate(sympy_symbols.items()):\n",
    "        symbol_namespace[f\"sym_{i}\"] = symbol\n",
    "        \n",
    "    # Add special values to the namespace\n",
    "    symbol_namespace[\"nan\"] = sympy.nan  # Handle 'nan' as sympy.nan\n",
    "    \n",
    "    # Create a reverse mapping for symbol replacement\n",
    "    symbol_replacements = {}\n",
    "    for i, name in enumerate(sympy_symbols.keys()):\n",
    "        # Use word boundaries to ensure we only replace whole symbols\n",
    "        symbol_replacements[r'\\b' + regex.escape(name) + r'\\b'] = f\"sym_{i}\"\n",
    "    \n",
    "    # Add special case for 'nan' (must be a whole word)\n",
    "    # This should NOT be replaced if it's part of a scientific notation\n",
    "    symbol_replacements[r'\\bnan\\b'] = \"nan\"  # Keep 'nan' as is since it's in namespace\n",
    "    \n",
    "    # Sort replacements by length (longest first) to avoid substring issues\n",
    "    sorted_replacements = sorted(\n",
    "        symbol_replacements.items(), \n",
    "        key=lambda x: len(x[0]), \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Prepared symbol mappings in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Split the expression into manageable chunks at + and - operators\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    # Use a faster single-pass approach for the initial split\n",
    "    while i < len(ginac_expr_str):\n",
    "        char = ginac_expr_str[i]\n",
    "        \n",
    "        # Check if this is scientific notation\n",
    "        is_sci_notation = False\n",
    "        if char in ['+', '-'] and i > 0:\n",
    "            # Look back to see if this is part of scientific notation\n",
    "            if ginac_expr_str[i-1].lower() == 'e' and i >= 2 and ginac_expr_str[i-2].isdigit():\n",
    "                is_sci_notation = True\n",
    "        \n",
    "        # Handle plus/minus at top level (not in scientific notation)\n",
    "        if char in ['+', '-'] and not is_sci_notation:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            current_chunk = char if char == '-' else ''\n",
    "        else:\n",
    "            current_chunk += char\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    print(f\"Split into {len(chunks)} chunks in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    chunk_size = 1000  # Adjust based on your system\n",
    "    batches = [chunks[i:i + chunk_size] for i in range(0, len(chunks), chunk_size)]\n",
    "    \n",
    "    results = []\n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Process this batch in parallel\n",
    "        n_processes = min(cpu_count(), len(batch))\n",
    "        if n_processes > 1:\n",
    "            with Pool(processes=n_processes) as pool:\n",
    "                process_func = partial(\n",
    "                    process_term, \n",
    "                    sorted_replacements=sorted_replacements,\n",
    "                    symbol_namespace=symbol_namespace\n",
    "                )\n",
    "                batch_results = pool.map(process_func, batch)\n",
    "        else:\n",
    "            # For small batches, avoid Pool overhead\n",
    "            batch_results = [\n",
    "                process_term(\n",
    "                    term, \n",
    "                    sorted_replacements=sorted_replacements,\n",
    "                    symbol_namespace=symbol_namespace\n",
    "                ) \n",
    "                for term in batch\n",
    "            ]\n",
    "        \n",
    "        # Combine results from this batch\n",
    "        batch_result = sum(batch_results)\n",
    "        results.append(batch_result)\n",
    "        \n",
    "        print(f\"Processed batch {batch_idx+1}/{len(batches)} in {time.time() - batch_start:.2f} seconds\")\n",
    "    \n",
    "    # Combine all batch results\n",
    "    final_result = sum(results)\n",
    "    print(f\"Total processing time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "def process_term(term, sorted_replacements, symbol_namespace):\n",
    "    \"\"\"\n",
    "    Process a single term from the expression.\n",
    "    Uses pre-computed symbol replacements for consistent handling.\n",
    "    \"\"\"\n",
    "    # Handle scientific notation and exponentiation\n",
    "    term = term.replace(\"^\", \"**\")\n",
    "    \n",
    "    # Process scientific notation in parentheses first\n",
    "    processed_term = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(term):\n",
    "        # Scientific notation in parentheses (e.g., (6.741051087966850411E-11))\n",
    "        if i < len(term) - 1 and term[i] == '(' and term[i+1].isdigit():\n",
    "            # Find the end of the parenthesized scientific notation\n",
    "            end_paren = term.find(')', i)\n",
    "            if end_paren != -1:\n",
    "                sci_notation = term[i+1:end_paren]\n",
    "                # Convert to SymPy Float for precision\n",
    "                processed_term += f\"sympy.Float('{sci_notation}')\"\n",
    "                i = end_paren + 1\n",
    "                continue\n",
    "        \n",
    "        # Regular character\n",
    "        processed_term += term[i]\n",
    "        i += 1\n",
    "    \n",
    "    # Handle special constants before symbol replacements\n",
    "    # Replace 'nan' that is not part of a scientific notation with sympy.nan\n",
    "    processed_term = regex.sub(r'\\bnan\\b', 'sympy.nan', processed_term)\n",
    "    \n",
    "    # Now apply all symbol replacements in order (longest first)\n",
    "    for pattern, replacement in sorted_replacements:\n",
    "        processed_term = regex.sub(pattern, replacement, processed_term)\n",
    "    \n",
    "    # Create namespace for evaluation\n",
    "    namespace = {\n",
    "        \"sympy\": sympy,\n",
    "        **symbol_namespace\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Compile to bytecode first\n",
    "        compiled_expr = compile(processed_term, '<string>', 'eval')\n",
    "        return eval(compiled_expr, {\"__builtins__\": {}}, namespace)\n",
    "    except Exception as e:\n",
    "        # More detailed error reporting\n",
    "        print(f\"Error processing term (first 50 chars): '{term[:50]}...': {e}\")\n",
    "        print(f\"Processed term was: '{processed_term[:100]}...'\")\n",
    "        # Return zero for this term but continue processing\n",
    "        return sympy.S.Zero\n",
    "\n",
    "# Helper function to check for other special mathematical constants\n",
    "def check_for_special_constants(expr_str):\n",
    "    \"\"\"\n",
    "    Identify special mathematical constants in the expression\n",
    "    \"\"\"\n",
    "    constants = {\n",
    "        'nan': 'Not a number',\n",
    "        'inf': 'Infinity',\n",
    "        'pi': 'Pi constant',\n",
    "        'e': 'Euler\\'s number',\n",
    "        # Add other constants if needed\n",
    "    }\n",
    "    \n",
    "    found = {}\n",
    "    for const in constants:\n",
    "        pattern = r'\\b' + const + r'\\b'\n",
    "        if regex.search(pattern, expr_str):\n",
    "            found[const] = constants[const]\n",
    "    \n",
    "    if found:\n",
    "        print(\"Found special constants:\")\n",
    "        for const, desc in found.items():\n",
    "            print(f\"  - {const}: {desc}\")\n",
    "    \n",
    "    return found\n",
    "\n",
    "# Optional: Add this debugging function to help identify symbol issues\n",
    "def debug_symbol_matching(expr_str, sympy_symbols):\n",
    "    \"\"\"Debug which symbols are found and which are missing\"\"\"\n",
    "    # Find all potential symbol patterns\n",
    "    symbol_pattern = r'\\be[a-zA-Z]?[0-9]+\\b'\n",
    "    found_symbols = set(regex.findall(symbol_pattern, expr_str))\n",
    "    \n",
    "    # Check which ones are in the provided dictionary\n",
    "    missing = [s for s in found_symbols if s not in sympy_symbols]\n",
    "    present = [s for s in found_symbols if s in sympy_symbols]\n",
    "    \n",
    "    print(f\"Total potential symbols found: {len(found_symbols)}\")\n",
    "    print(f\"Symbols in dictionary: {len(present)}\")\n",
    "    print(f\"Symbols not in dictionary: {len(missing)}\")\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"First 10 missing symbols: {missing[:10]}\")\n",
    "    \n",
    "    # Check for case-sensitivity issues\n",
    "    sympy_keys_lower = [k.lower() for k in sympy_symbols.keys()]\n",
    "    case_issues = [s for s in missing if s.lower() in sympy_keys_lower]\n",
    "    \n",
    "    if case_issues:\n",
    "        print(f\"Possible case-sensitivity issues: {case_issues[:10]}\")\n",
    "\n",
    "# Additional utility function for memory tracking (optional)\n",
    "def get_memory_usage():\n",
    "    \"\"\"Return current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a51e359-82c8-4033-a039-71e64afa982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zorro(ss, X_train, y_train, X_test, y_test, robustness_radius, uncertain_attr, \n",
    "                                 uncertain_num, uncertain_radius=None, uncertain_radius_ratio=None, \n",
    "                                 lr=0.03, reg=0, seed=42):\n",
    "    start_time = time.perf_counter()\n",
    "    X = copy.deepcopy(X_train)\n",
    "    y = copy.deepcopy(y_train)\n",
    "    \n",
    "    symbol_id = -1\n",
    "    symbolic_data = X.tolist()\n",
    "    symbols_in_data = set()\n",
    "    symbol_to_position = dict()\n",
    "    symbols_dict = dict()\n",
    "    XS = X.tolist()\n",
    "    XR = X.tolist()\n",
    "    for row in range(len(symbolic_data)):\n",
    "        for col in range(len(symbolic_data[0])):\n",
    "            xmin = X_extended_min[row][col]\n",
    "            xmax = X_extended_max[row][col]\n",
    "            if xmin != xmax:\n",
    "                xmean = (xmax + xmin) / 2\n",
    "                xradius = (xmax - xmin) / 2\n",
    "                new_symbol = create_symbol()\n",
    "                symbolic_data[row][col] = xmean + xradius*new_symbol\n",
    "                XS[row][col] = xradius*new_symbol\n",
    "                XR[row][col] = xmean\n",
    "                symbols_in_data.add(new_symbol)\n",
    "                symbol_to_position[new_symbol] = (row, col)\n",
    "                symbols_dict[str(new_symbol)] = new_symbol\n",
    "            else:\n",
    "                XS[row][col] = 0\n",
    "                \n",
    "    XS = sympy.Matrix(XS)\n",
    "    XR = sympy.Matrix(XR)\n",
    "    n = XS.shape[0]\n",
    "    n1 = XS.shape[1]\n",
    "    yR = sympy.Matrix(y_train)\n",
    "    yS = yR*0.0\n",
    "    y_test_orig = y_test.copy()\n",
    "    y_test = sympy.Matrix(y_test.to_numpy().reshape(-1, 1))\n",
    "    # X_test = sympy.Matrix(np.append(np.ones((len(X_test), 1)), X_test, axis=1))\n",
    "    X_test = sympy.Matrix(np.append(np.ones((len(X_test), 1)), ss.transform(X_test), axis=1))\n",
    "\n",
    "    # for row in range(n):\n",
    "    #     for col in range(n1):\n",
    "    #         expr = copy.deepcopy(XR[row, col])\n",
    "    #         if isinstance(expr, sympy.Expr) and expr.free_symbols:\n",
    "    #             XR[row, col] = expr.subs(dict([(symb, 0) for symb in expr.free_symbols]))\n",
    "    #             XS[row, col] = expr - XR[row, col]\n",
    "    #         else:\n",
    "    #             XR[row, col] = expr\n",
    "    #             XS[row, col] = 0\n",
    "\n",
    "    # for row in range(yR.shape[0]):\n",
    "    #     expr = copy.deepcopy(yR[row])\n",
    "    #     if isinstance(expr, sympy.Expr) and expr.free_symbols:\n",
    "    #         yR[row] = expr.subs(dict([(symb, 0) for symb in expr.free_symbols]))\n",
    "    #         yS[row] = expr - yR[row]\n",
    "    #     else:\n",
    "    #         yR[row] = expr\n",
    "    #         yS[row] = 0\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Data loading execution time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    common_inv = (XR.T*XR + reg*n*np.identity(X.shape[1])).inv()\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Common inverse matrix computation time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    V, sigma, VT = np.linalg.svd(np.array((XR.T*XR).tolist()).astype(float))\n",
    "    V = sympy.Matrix(V)\n",
    "    VT = sympy.Matrix(VT)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"SVD computation time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    wR = common_inv*XR.T*yR\n",
    "    wS_data = common_inv*((XS.T*XR + XR.T*XS)*wR - XS.T*yR - XR.T*yS)\n",
    "    wS_non_data = 0.0*VT.row(0).T\n",
    "    for i in range(X.shape[1]):\n",
    "        wS_non_data = wS_non_data + sb(f'k{i}')*sb(f'ep{i}')*VT.row(i).T\n",
    "        symbols_dict[f\"ep{i}\"] = sb(f'ep{i}')\n",
    "        symbols_dict[f\"k{i}\"] = sb(f'k{i}')\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Abstract weights computation time: {execution_time} seconds\")\n",
    "    print(symbols_dict)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    eigenvalues = 1 - 2*lr*reg - 2*lr*sigma/n\n",
    "    for eigenvalue in eigenvalues:\n",
    "        assert eigenvalue <= 1\n",
    "        assert eigenvalue >= 0\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Eigenvalue computation time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    wS = wS_non_data + wS_data\n",
    "    w = wS + wR\n",
    "    w_prime = (-lr*2/n)*((XS.T*XR + XR.T*XS + XS.T*XS)*wS + XS.T*XS*wR - XS.T*yS).expand()\n",
    "    # w_prime = (XS.T*XR + XR.T*XS + XS.T*XS)*wS + XS.T*XS*wR - XS.T*yS\n",
    "    # print(\"Before expansion:\", w_prime.rows, w_prime.cols)\n",
    "    # w_prime = ginac_matrix_to_sympy(ginac_module.expand_matrix(sympy_matrix_to_ginac(w_prime), w_prime.rows, w_prime.cols), w_prime.rows, w_prime.cols, symbols_dict)\n",
    "    # print(\"After expansion:\", w_prime.rows, w_prime.cols)\n",
    "    # w_prime = (-lr*2/n)*w_prime\n",
    "    # print(\"w_prime final:\", w_prime.rows, w_prime.cols, \"\\n\")\n",
    "    \n",
    "    coeff_cont = (-lr*2/n)*((XS.T*XR + XR.T*XS + XS.T*XS)*wS_non_data).expand()\n",
    "    # coeff_cont = (XS.T*XR + XR.T*XS + XS.T*XS)*wS_non_data\n",
    "    # print(\"Before expansion:\", coeff_cont.rows, coeff_cont.cols)\n",
    "    # coeff_cont = ginac_matrix_to_sympy(ginac_module.expand_matrix(sympy_matrix_to_ginac(coeff_cont), coeff_cont.rows, coeff_cont.cols), coeff_cont.rows, coeff_cont.cols, symbols_dict)\n",
    "    # print(\"After expansion:\", coeff_cont.rows, coeff_cont.cols)\n",
    "    # coeff_cont = (-lr*2/n)*coeff_cont\n",
    "    # print(\"coeff_cont final:\", coeff_cont.rows, coeff_cont.cols)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"W# (abstract gradient descent?) computation time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    w_prime_projected = VT*w_prime\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Projection computation time: {execution_time} seconds\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    eqs = []\n",
    "    for d in tqdm(range(X.shape[1]), desc='Equation'):\n",
    "        eq1 = (1-abs(eigenvalues[d]))*sb(f'k{d}')\n",
    "        eq2 = 0\n",
    "        coef_dict = dict()\n",
    "        coef_dict['const'] = 0\n",
    "        for i in range(X.shape[1]):\n",
    "            coef_dict[sb(f'k{i}')] = 0\n",
    "        for arg in w_prime_projected[d].args:\n",
    "            contain_k = False\n",
    "            for i in range(X.shape[1]):\n",
    "                symb_k = sb(f'k{i}')\n",
    "                if symb_k in arg.free_symbols:\n",
    "                    coef_dict[symb_k] = coef_dict[symb_k] + abs(arg.args[0])\n",
    "                    contain_k = True\n",
    "                    break\n",
    "            if not(contain_k):\n",
    "                coef_dict['const'] = coef_dict['const'] + abs(arg.args[0])\n",
    "        eq2 = coef_dict['const']\n",
    "        for i in range(X.shape[1]):\n",
    "            eq2 = eq2 + sb(f'k{i}')*coef_dict[sb(f'k{i}')]\n",
    "        eqs.append(sympy.Eq(eq1, eq2))\n",
    "        \n",
    "    result = sympy.solve(eqs, [sb(f'k{i}') for i in range(X.shape[1])])\n",
    "    print(result)\n",
    "    for ki in result:\n",
    "        assert result[ki] >= 0\n",
    "    param = wR + wS.subs(result)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Equation solving time: {execution_time} seconds\")\n",
    "    \n",
    "    # center_preds = X_test*get_expr_center(param)\n",
    "    # print('one-poss-world:', [((center_preds - y_test).T*(center_preds - y_test)/n)[0]])\n",
    "    # start_time = time.perf_counter()\n",
    "    # test_preds = X_test*param\n",
    "    # end_time = time.perf_counter()\n",
    "    # execution_time = end_time - start_time\n",
    "    # print(f\"Prediction time: {execution_time} seconds\")\n",
    "    # print(test_preds[0])\n",
    "    # robustness_ls = []\n",
    "#     se_min_ls = []\n",
    "#     se_max_ls = []\n",
    "    # acc_count_low = 0\n",
    "    # acc_count_high = 0\n",
    "    # for pred_id in tqdm(range(len(test_preds)), desc='Testing'):\n",
    "    #     pred = test_preds[pred_id]\n",
    "    #     label_test = y_test[pred_id]\n",
    "    #     pred_range_radius = get_expr_range_radius(pred)\n",
    "    #     if pred_range_radius <= robustness_radius:\n",
    "    #         robustness_ls.append(1)\n",
    "    #     else:\n",
    "    #         robustness_ls.append(0)\n",
    "    #     pred_center = get_expr_center(pred)\n",
    "    #     pred_low = pred_center - pred_range_radius\n",
    "    #     pred_hi = pred_center + pred_range_radius\n",
    "    #     # get accuracy lower and upper bound\n",
    "    #     if pred_hi < 0.5 and label_test == 0:\n",
    "    #         acc_count_low += 1\n",
    "    #         acc_count_high += 1\n",
    "    #     elif pred_low > 0.5 and label_test == 1:\n",
    "    #         acc_count_low += 1\n",
    "    #         acc_count_high += 1\n",
    "    #     elif pred_low < 0.5 and pred_hi > 0.5:\n",
    "    #         acc_count_high += 1\n",
    "#         rewrite_pred = pred_center+pred_range_radius*sb('err_pred')\n",
    "#         se_min, se_max = min_max_sympy_expression((rewrite_pred-label_test)**2)\n",
    "#         se_min_ls.append(se_min)\n",
    "#         se_max_ls.append(se_max)\n",
    "    # preds_diff = (test_preds - y_test).T*(test_preds - y_test)/n\n",
    "    start_time = time.perf_counter()\n",
    "    ginac_param = sympy_to_ginac_format(param)\n",
    "    X_test_list = list(np.array(X_test, dtype=float))\n",
    "    X_test_list = [list(l) for l in X_test_list]\n",
    "    y_test_list = [float(l[0]) for l in y_test_orig.to_numpy()]\n",
    "    ginac_param_list = [sympy_to_ginac_format(s) for s in param]\n",
    "    abstract_loss_str = ginac_module.abstract_loss(X_test_list, y_test_list, ginac_param_list)\n",
    "\n",
    "    POWER_PATTERN = regex.compile(r'\\^')\n",
    "    math_funcs = {\n",
    "        'sin': sympy.sin, \n",
    "        'cos': sympy.cos, \n",
    "        'tan': sympy.tan,\n",
    "        'exp': sympy.exp, \n",
    "        'log': sympy.log, \n",
    "        'sqrt': sympy.sqrt\n",
    "    }\n",
    "    expr_str = POWER_PATTERN.sub('**', abstract_loss_str[0])\n",
    "    \n",
    "    # preds_diff_full = eval(expr_str, {\"__builtins__\": {}}, {**symbols_dict, **math_funcs})\n",
    "    # compiled_expr = compile(expr_str, '<string>', 'eval')\n",
    "    # preds_diff_full = eval(compiled_expr, symbols_dict)\n",
    "    preds_diff_full = parse_ginac_tokens(abstract_loss_str[0], symbols_dict)\n",
    "    end_time = time.perf_counter()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Loss computation and expansion time: {execution_time} seconds\")\n",
    "    \n",
    "    # start_time = time.perf_counter()\n",
    "    # n = X_test.shape[0]\n",
    "    # preds_diff = ((test_preds - y_test).T*(test_preds - y_test)/n)[0]\n",
    "    # end_time = time.perf_counter()\n",
    "    # execution_time = end_time - start_time\n",
    "    # print(f\"Loss computation time: {execution_time} seconds\")\n",
    "\n",
    "    # start_time = time.perf_counter()\n",
    "    # preds_diff_full = linearization([preds_diff])[0]\n",
    "    # preds_diff_center = get_expr_center(preds_diff)\n",
    "    # preds_diff_radius = get_expr_range_radius(preds_diff)\n",
    "#     print(preds_diff_center)\n",
    "#     print(preds_diff_radius)\n",
    "    # print(\"Robustness Ratio: \" + str(np.mean(robustness_ls)))\n",
    "    \n",
    "#     print(param)\n",
    "    # end_time = time.perf_counter()\n",
    "    # execution_time = end_time - start_time\n",
    "    # print(f\"Loss linearization time: {execution_time} seconds\")\n",
    "    return result, param, wS_data, wS_non_data, wR, w_prime, w_prime_projected, preds_diff_full, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aed7ad-bc7d-4e0f-a711-2db983195f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expr_range_radius(expr):\n",
    "    expr_range_radius = 0\n",
    "    for arg in expr.args:\n",
    "        if arg.free_symbols:\n",
    "            expr_range_radius += abs(arg.args[0])\n",
    "    return expr_range_radius\n",
    "\n",
    "def get_expr_center(expr):\n",
    "    return expr.subs(dict([(symb, 0) for symb in expr.free_symbols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4647b8-daeb-40c6-90f6-c982f81b220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('dataset/Mines_injector_lr/real/X_train_clean.csv')\n",
    "X_train_dirty = pd.read_csv('dataset/Mines_injector_lr/real/X_train_dirty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60dd9f0a-6460-47f2-ac56-9dd0db315e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACYAAAAQCAYAAAB6Hg0eAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAASdAAAEnQB3mYfeAAAAg1JREFUeJzN1T9olWcUx/FPYiaLpBDIYodKUJd2Eyw6BUFEQdDSUYtboTVGOgiCHI4QcFKD0qVCAm6Fgh0smkUc6p9CoNBia7QlmyL+RXTQ1nR4n6uvr/eam7voWX73Pvc553vuec97Tt/CwoL30QZaHzJzCDuwDZ9iJZ7hd0xhKiJeNANk5kc4jC0Ywi2cQUbEg8bdrhn9Nb8v8D3W4yqO40d8glP4ITP7GqARzGIPfsUx/IN9uFwSqVvXjIGa0xy242y9Mpl5sEA/x84SqGXfYRhjEXGi5nMU+zGBr3ph9HXTY8VxAicjYm+tWjcxj5EGaIXqkfZhOCKeLJXRv5hDsedF/62djRadafZeRDzGL1iOz3phLJpYZg5gd/l6rvbT2qJzHVxvFF3TC6Obih1RNefPEXG+dj5Y9FEHv9b5h70w3ppYZo7hW/yFXV0AlmydGB0Ty8xvMIlrGI2I+40rrYoMam+t84e9MNomlpnjOIE/isPtNteuF+3UQ6uLtu3BxRhvJJaZB1SD8rficKcD+ELRzZn5WpwyLjbiKa70wmgGPKRqxFlsioi7HZISEX9jBh/j6yYbH+B0c4Z1y3g5YDPzS0zjP1WJ271t8xExXYOM4JJq+v+EP1XrZlT1CDdExL3a/a4Z9ZW0qugyjLf7F7hYAqOqWmau82qJb1VN/EltlvhSGF2tpHdh/wMTB+0FNn8+6wAAAABJRU5ErkJggg==",
      "text/latex": [
       "$\\displaystyle 202$"
      ],
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv('dataset/Mines_injector_lr/real/y_train.csv')\n",
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce604cb6-d13b-4aef-b3a2-2d8c3f52a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     charges\n",
       "0        NaN\n",
       "1        NaN\n",
       "2        NaN\n",
       "3        NaN\n",
       "4        NaN\n",
       "..       ...\n",
       "197      NaN\n",
       "198      NaN\n",
       "199      NaN\n",
       "200      NaN\n",
       "201      NaN\n",
       "\n",
       "[202 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.DataFrame(y_train, columns=['charges'])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5981621-9479-4a2d-a82c-f52805b197bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "def compute_closed_form(X, y):\n",
    "    return np.matmul(np.linalg.inv(np.matmul(X.T, X)), np.matmul(X.T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d9193c-ea17-4e9b-b4ce-19aa57704114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_df = X_train_dirty.merge(y_train, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ebacf3f-b266-4c61-a11a-52f1afdb4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = ['V', 'H', 'S']\n",
    "all_cols_idx = [df.columns.to_list().index(c) for c in all_cols]\n",
    "\n",
    "# num of errors injected\n",
    "# mv_num = 30\n",
    "\n",
    "# which col to inject missing val\n",
    "treatment_idx = all_cols.index('V')\n",
    "uncertain_attr = all_cols.index('S')\n",
    "# uncertain_attr = [all_cols.index('displacement'), all_cols.index('weight')]\n",
    "\n",
    "# MNAR\n",
    "# def non_random_pattern(data_X, data_y):\n",
    "#     binary_indicators = []\n",
    "#     for i in range(data_X.shape[0]):\n",
    "#         if (data_X.iloc[i, treatment_idx] > 25) and (data_X.iloc[i, uncertain_attr] > 0):\n",
    "#             binary_indicators.append(1)\n",
    "#         else:\n",
    "#             binary_indicators.append(0)\n",
    "#     return np.array(binary_indicators)\n",
    "\n",
    "# sample_pattern_len = np.sum(non_random_pattern(df.drop('cylinders', axis=1).copy(), df.cylinders))\n",
    "# mv_ratio = mv_num/sample_pattern_len\n",
    "# print(mv_ratio)\n",
    "# mv_err = MissingValueError(uncertain_attr, pattern=non_random_pattern, ratio=mv_ratio)\n",
    "# dirty_df, dirty_y, _, _ = mv_err.inject(df.drop('cylinders', axis=1).copy(), df.cylinders, df.drop('cylinders', axis=1), df.cylinders)\n",
    "\n",
    "X_extended = np.append(np.ones((len(dirty_df), 1)), \n",
    "                       dirty_df.to_numpy().astype(float)[:, all_cols_idx], axis=1)\n",
    "X_extended_clean = np.append(np.ones((len(df), 1)), \n",
    "                             df.to_numpy().astype(float)[:, all_cols_idx], axis=1)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_extended[:, 1:] = ss.fit_transform(X_extended[:, 1:])\n",
    "X_extended_clean[:, 1:] = ss.transform(X_extended_clean[:, 1:])\n",
    "\n",
    "param_clean = compute_closed_form(X_extended_clean, y_train)\n",
    "\n",
    "# first column becomes constant 1's\n",
    "treatment_idx += 1\n",
    "uncertain_attr += 1\n",
    "# remain_col_idx = [0] + [i+1 for i in remain_col_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d448a07-5dd9-420c-8962-c23242015423",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = [KNNImputer(n_neighbors=3), KNNImputer(n_neighbors=5), KNNImputer(n_neighbors=10), IterativeImputer(random_state=42)]\n",
    "num_attrs = X_extended.shape[1]\n",
    "X_nan = X_extended.copy()\n",
    "imputed_cols = [X_extended_clean[:, uncertain_attr]]\n",
    "# imputed_other = [X_extended_clean[:, other_uncertain]]\n",
    "imputed_datasets = [X_extended_clean]\n",
    "for imp in imputers:\n",
    "    imputed_dataset = imp.fit_transform(X_nan)\n",
    "    imputed_datasets.append(imputed_dataset)\n",
    "    imputed_cols.append(imputed_dataset[:, uncertain_attr])\n",
    "    # imputed_other.append(imputed_dataset[:, other_uncertain])\n",
    "\n",
    "X_extended_max = X_extended.copy()\n",
    "X_extended_max[:, uncertain_attr] = np.max(imputed_cols, axis=0)\n",
    "# X_extended_max[:, other_uncertain] = np.max(imputed_other, axis=0)\n",
    "# X_extended_max = X_extended_max[:, remain_col_idx]\n",
    "\n",
    "X_extended_min = X_extended.copy()\n",
    "X_extended_min[:, uncertain_attr] = np.min(imputed_cols, axis=0)\n",
    "# X_extended_min[:, other_uncertain] = np.min(imputed_other, axis=0)\n",
    "# X_extended_min = X_extended_min[:, remain_col_idx]\n",
    "\n",
    "# X_extended = X_extended[:, remain_col_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b361ac-5e5e-4400-8eb8-bd356f9e2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('dataset/Mines_injector_lr/real/X_test.csv')\n",
    "y_test = pd.read_csv('dataset/Mines_injector_lr/real/y_test.csv')\n",
    "dirty_y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec68b87-eadd-45be-9228-0286283ca4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.read_csv('dataset/Mines_injector_lr/real/X_val.csv')\n",
    "y_val = pd.read_csv('dataset/Mines_injector_lr/real/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dac21be-a144-4313-8df4-d9c815b7c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.concat([X_train, X_train_dirty])\n",
    "df_diff = df_diff.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddcc64c7-3113-4116-a63a-4ba49562f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  15,  17,  22,  35,  50,  55,  66,  70,  83,  92, 107, 111,\n",
       "       115, 117, 125, 131, 139, 143, 151, 155, 156, 167, 172])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injected_indices = []\n",
    "for i in range(len(dirty_df)):\n",
    "    if pd.isna(dirty_df.iloc[i]['S']):\n",
    "        injected_indices.append(i)\n",
    "\n",
    "injected_indices = np.array(injected_indices)\n",
    "injected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7750f99-1353-4b6d-bb60-3c26160e414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABoAAAAPCAYAAAD6Ud/mAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAASdAAAEnQB3mYfeAAAAZpJREFUeJy91M+LTXEYx/HXHfcPUJaysFQs2MzGghEbpZCdHys73W5Ramo8HqVmI6YslNHI7NTYERvlR02UUiSsZCX5sVHy81qcM3Uc5547LDyb7/mez/d83s9znqdvZzAY+B/RXXrIzFXYjZ3YgNX4iieYw1xE/Gwzy8z9mC+3hyNidkkbq5zbh4sYxwOcwwLWYxZXM7PTAlmD8/jUWhFeYheuVzPPzEk8xF7sKeF1SEdR9Xtcw7GhoIi43ZRJRLzJzAs4jS1NIPQwUeoTTT5jTS8b4lu5fq8LmbkO05iJiLvDDEaCMrOLg+X2ZoM2j9eYbPPptollTCsG4kZE3KppJ7ARmyPic5tJa0WZ2cNRPMeBmjauqOJMRCyOynYoKDOPYAbPsDUiPlS0Lq4oJnVqFAQ6TTdDZvZxFk+xLSLe1vSV+LgcgGJI+n/0KDOPK/ryGNsj4l3Dx19waYjxJkXf7uMFFqlVlJlTOIVH2FH9XcuNzDyJULuCqnfdoRLyA/fQy8y6z6uIuPy38N9AWFuuK9Afcv4O/gn0C/mfhw9MchEQAAAAAElFTkSuQmCC",
      "text/latex": [
       "$\\displaystyle 24$"
      ],
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injected_size = len(injected_indices)\n",
    "injected_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b63169-4e64-4412-8929-f4e08ddb2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading execution time: 0.043603964000794804 seconds\n",
      "Common inverse matrix computation time: 0.022781926996685797 seconds\n",
      "SVD computation time: 0.014995888999692397 seconds\n",
      "Abstract weights computation time: 0.09894620900013251 seconds\n",
      "{'e0': e0, 'e1': e1, 'e2': e2, 'e3': e3, 'e4': e4, 'e5': e5, 'e6': e6, 'e7': e7, 'e8': e8, 'e9': e9, 'e10': e10, 'e11': e11, 'e12': e12, 'e13': e13, 'e14': e14, 'e15': e15, 'e16': e16, 'e17': e17, 'e18': e18, 'e19': e19, 'e20': e20, 'e21': e21, 'e22': e22, 'e23': e23, 'ep0': ep0, 'k0': k0, 'ep1': ep1, 'k1': k1, 'ep2': ep2, 'k2': k2, 'ep3': ep3, 'k3': k3}\n",
      "Eigenvalue computation time: 4.651199924410321e-05 seconds\n",
      "W# (abstract gradient descent?) computation time: 0.2333285739987332 seconds\n",
      "Projection computation time: 0.00020747700182255358 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba35bf9b7ad494d838ad88c4af69c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Equation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{k0: 0.0, k1: 0.0, k2: 0.0, k3: 0.0}\n",
      "Equation solving time: 0.06241954199867905 seconds\n",
      "X: 68*4\n",
      "Prepared symbol mappings in 0.00 seconds\n",
      "Split into 4 chunks in 0.00 seconds\n",
      "y: 68*1\n",
      "param: 4*1\n",
      "Time for multiplication: 1 ms\n",
      "Time for difference: 0 ms\n",
      "Time for square and expand: 0 ms\n",
      "Number of nodes in sum_squared: 58\n",
      "Number of terms in sum_squared: 15\n",
      "Depth of sum_squared: 3\n",
      "Current memory usage: \t  353064 kB\n",
      "Symbol extraction time: 0 ms\n",
      "Symbol priority generation time: 0 ms\n",
      "combine_like_terms_robust() time: 0 ms\n",
      "Time for combining like terms: 0 ms\n",
      "Number of nodes in sum_squared after expansion: 14\n",
      "Number of terms in sum_squared after expansion: 4\n",
      "Depth of sum_squared after expansion: 3\n",
      "Current memory usage: \t  353064 kB\n",
      "Processed batch 1/1 in 0.04 seconds\n",
      "Total processing time: 0.04 seconds\n",
      "Loss computation and expansion time: 0.04123687099854578 seconds\n"
     ]
    }
   ],
   "source": [
    "result, param, wS_data, wS_non_data, wR, w_prime, w_prime_projected, preds_diff_full, X_test_matrix = zorro(ss, X_extended, dirty_y, X_val, y_val, 0.05, uncertain_attr, \n",
    "                             injected_size, uncertain_radius=None, uncertain_radius_ratio=None, lr=0.01, reg=0, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a094be-7f03-4931-a90e-fae3e0a9bfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
